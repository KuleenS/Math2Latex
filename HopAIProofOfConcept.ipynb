{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "#add more parameters\n",
    "#add a confusion matrix\n",
    "#mess around with hyperparameters\n",
    "#related research - increasing kernel size \n",
    "#grid search/ray tune \n",
    "\n",
    "#data augementation \n",
    "#mnist \n",
    "#noise \n",
    "#-30 to 30 degree rotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing data\n",
    "transform = transforms.Compose([\n",
    "transforms.ToTensor(), #turns JPG to Tensor Object\n",
    "transforms.Grayscale(num_output_channels=1), #turns the 3 layers RGB to 1 layer black and white,\n",
    "transforms.Normalize(mean=[0.5,], std=[0.5,]) #some nornmalization of the grayscale,\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataset object of the file system\n",
    "data = torchvision.datasets.ImageFolder(root='./archive/data/extracted_images/', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making the splits for the dataset\n",
    "#taking 75% as train and 25% as hold out for test\n",
    "trainsize = int(len(data)*0.75)\n",
    "testsize = len(data)-trainsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting randomly our dataset into our chunks\n",
    "train_set, test_set = torch.utils.data.random_split(data, (trainsize,testsize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the dataloader object (basically an iterator through our dataset that batches our files together) \n",
    "train_data_loader = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True,  num_workers=1)\n",
    "test_data_loader = torch.utils.data.DataLoader(test_set, batch_size=32, shuffle=True,  num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining our ConvNet\n",
    "class Net(nn.Module):   \n",
    "  def __init__(self):\n",
    "      super(Net, self).__init__()\n",
    "\n",
    "      self.cnn_layers = nn.Sequential(\n",
    "          # Defining a 2D convolution layer\n",
    "          nn.Conv2d(1, 4, kernel_size=3, stride=1, padding=1),\n",
    "          nn.BatchNorm2d(4),\n",
    "          nn.ReLU(inplace=True),\n",
    "          nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "          # Defining another 2D convolution layer\n",
    "          nn.Conv2d(4, 4, kernel_size=3, stride=1, padding=1),\n",
    "          nn.BatchNorm2d(4),\n",
    "          nn.ReLU(inplace=True),\n",
    "          nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "      )\n",
    "\n",
    "      self.linear_layers = nn.Sequential(\n",
    "          nn.Linear(484, 82)\n",
    "      )\n",
    "\n",
    "  # Defining the forward pass    \n",
    "  def forward(self, x):\n",
    "      x = self.cnn_layers(x)\n",
    "      x = x.view(x.size(0), -1)\n",
    "      x = self.linear_layers(x)\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (cnn_layers): Sequential(\n",
      "    (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (linear_layers): Sequential(\n",
      "    (0): Linear(in_features=484, out_features=82, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "# defining the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "# defining the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# checking if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    #put our loss and model on GPU\n",
    "    #this makes it faster as GPUs have lots of multiprocessing capabilities and deal well with Matrix Multiplication and vectors \n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "    \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8812/8812 [02:21<00:00, 62.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.28207274171585\n",
      "Epoch 1 - Training loss: 0.33818531701936505\n",
      "Epoch  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8812/8812 [02:09<00:00, 68.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.32645823876531\n",
      "Epoch 2 - Training loss: 0.298522842145084\n",
      "Epoch  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8812/8812 [02:10<00:00, 67.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.8484736722651\n",
      "Epoch 3 - Training loss: 0.2772558693683364\n",
      "Epoch  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8812/8812 [02:11<00:00, 66.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.27261121198366\n",
      "Epoch 4 - Training loss: 0.26278072483630455\n",
      "Epoch  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8812/8812 [02:06<00:00, 69.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.48290683159328\n",
      "Epoch 5 - Training loss: 0.2524350926451659\n",
      "Epoch  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8812/8812 [02:06<00:00, 69.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.66483204720835\n",
      "Epoch 6 - Training loss: 0.24497509573654347\n",
      "Epoch  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8812/8812 [02:06<00:00, 69.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.81342203812983\n",
      "Epoch 7 - Training loss: 0.2393688892665997\n",
      "Epoch  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8812/8812 [02:06<00:00, 69.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.97938890149796\n",
      "Epoch 8 - Training loss: 0.23270885547733694\n",
      "Epoch  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8812/8812 [02:06<00:00, 69.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93.11521221062188\n",
      "Epoch 9 - Training loss: 0.22743987918363634\n",
      "Epoch  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8812/8812 [02:06<00:00, 69.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93.32125226963231\n",
      "Epoch 10 - Training loss: 0.2232431317265287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#train for 10 epochs\n",
    "from tqdm import tqdm \n",
    "for i in range(10):\n",
    "    print(\"Epoch \", i+1)\n",
    "    #creating loss per epoch and correct in training set \n",
    "    running_loss = 0\n",
    "    correct = 0\n",
    "    #loop through our dataloader\n",
    "    for images, labels in tqdm(train_data_loader):\n",
    "        #put data on GPU\n",
    "        if torch.cuda.is_available():\n",
    "          images = images.cuda()\n",
    "          labels = labels.cuda()\n",
    "\n",
    "        # Training pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #forward pass through our model\n",
    "        output = model(images)\n",
    "        #get the predicitions for that batch\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        #calculate the loss \n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        #This is where the model learns by backpropagating\n",
    "        loss.backward()\n",
    "        \n",
    "        #And optimizes its weights here\n",
    "        optimizer.step()\n",
    "        #keep track of our loss and sum\n",
    "        running_loss += loss.item()\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * (correct / (len(train_data_loader)*32))\n",
    "    print(accuracy)\n",
    "    print(\"Epoch {} - Training loss: {}\".format(i+1, running_loss/len(train_data_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test loop\n",
    "#pretty much the same thing as the dev but we do not back prop the error\n",
    "y_pred = []\n",
    "y_true = []\n",
    "correct_count, all_count = 0, 0\n",
    "with torch.no_grad():\n",
    "  for inputs, labels in test_data_loader:\n",
    "    inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    output = model(inputs) # Feed Network\n",
    "\n",
    "    output = (torch.max(torch.exp(output), 1)[1]).data.cpu().numpy()\n",
    "    y_pred.extend(output) # Save Prediction\n",
    "    \n",
    "    labels = labels.data.cpu().numpy()\n",
    "    y_true.extend(labels) # Save Tru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fa706d29148685102f4d2caf4b4a1631bca60a8faf710f50a41d3fdd530c8e64"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('reg': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
